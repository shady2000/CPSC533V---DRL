{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 533V: Assignment 3 - Behavioral Cloning and Deep Q Learning\n",
    "\n",
    "## 48 points total (9% of final grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This assignment will help you transition from tabular approaches, topic of HW 2, to deep neural network approaches. You will implement the [Atari DQN / Deep Q-Learning](https://arxiv.org/abs/1312.5602) algorithm, which arguably kicked off the modern Deep Reinforcement Learning craze.\n",
    "\n",
    "In this assignment we will use PyTorch as our deep learning framework.  To familiarize yourself with PyTorch, your first task is to use a behavior cloning (BC) approach to learn a policy.  Behavior cloning is a supervised learning method in which there exists a dataset of expert demonstrations (state-action pairs) and the goal is to learn a policy $\\pi$ that mimics this expert.  At any given state, your policy should choose the same action the export would.\n",
    "\n",
    "Since BC avoids the need to collect data from the policy you are trying to learn, it is relatively simple. \n",
    "This makes it a nice stepping stone for implementing DQN. Furthermore, BC is relevant to modern approaches---for example its use as an initialization for systems like [AlphaGo][go] and [AlphaStar][star], which then use RL to further adapte the BC result.  \n",
    "\n",
    "<!--\n",
    "\n",
    "I feel like this might be better suited to going lower in the document:\n",
    "\n",
    "Unfortunately, in many tasks it is impossible to collect good expert demonstrations, making\n",
    "\n",
    "it's not always possible to have good expert demonstrations for a task in an environemnt and this is where reinforcement learning comes handy. Through the reward signal retrieved by interacting with the environment, the agent learns by itself what is a good policy and can learn to outperform the experts.\n",
    "\n",
    "-->\n",
    "\n",
    "Goals:\n",
    "- Famliarize yourself with PyTorch and its API including models, datasets, dataloaders\n",
    "- Implement a supervised learning approach (behavioral cloning) to learn a policy.\n",
    "- Implement the DQN objective and learn a policy through environment interaction.\n",
    "\n",
    "[go]:  https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
    "[star]: https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii\n",
    "\n",
    "## Submission information\n",
    "\n",
    "- Complete the assignment by editing and executing the associated Python files.\n",
    "- Copy and paste the code and the terminal output requested in the predefined cells on this Jupyter notebook.\n",
    "- When done, upload the completed Jupyter notebook (ipynb file) on canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Preliminaries\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "If you have never used PyTorch before, we recommend you follow this [60 Minutes Blitz][blitz] tutorial from the official website. It should give you enough context to be able to complete the assignment.\n",
    "\n",
    "\n",
    "**If you have issues, post questions to Piazza**\n",
    "\n",
    "### Installation\n",
    "\n",
    "To install all required python packages:\n",
    "\n",
    "```\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Debugging\n",
    "\n",
    "\n",
    "You can include:  `import ipdb; ipdb.set_trace()` in your code and it will drop you to that point in the code, where you can interact with variables and test out expressions.  We recommend this as an effective method to debug the algorithms.\n",
    "\n",
    "\n",
    "[blitz]: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Behavioral Cloning\n",
    "\n",
    "Behavioral Cloning is a type of supervised learning in which you are given a dataset of expert demonstrations tuple $(s, a)$ and the goal is to learn a policy function $\\hat a = \\pi(s)$, such that $\\hat a = a$.\n",
    "\n",
    "The optimization objective is $\\min_\\theta D(\\pi(s), a)$ where $\\theta$ are the parameters the policy $\\pi$, in our case the weights of a neural network, and where $D$ represents some difference between the actions.\n",
    "\n",
    "---\n",
    "\n",
    "Before starting, we suggest reading through the provided files.\n",
    "\n",
    "For Behavioral Cloning, the important files to understand are: `model.py`, `dataset.py` and `bc.py`.\n",
    "\n",
    "- The file `model.py` has the skeleton for the model (which you will have to complete in the following questions),\n",
    "\n",
    "- The file `dataset.py` has the skeleton for the dataset the model is being trained with,\n",
    "\n",
    "- and, `bc.py` will have all the structure for training the model with the dataset.\n",
    "\n",
    "\n",
    "### 1.1 Dataset\n",
    "\n",
    "We provide a pickle file with pre-collected expert demonstrations on CartPole from which to learn the policy $\\pi$. The data has been collected from an expert policy on the environment, with the addition of a small amount of gaussian noise to the actions.\n",
    "\n",
    "The pickle file contains a list of tuples of states and actions in `numpy` in the following way:\n",
    "\n",
    "```\n",
    "[(state s, action a), (state s, action a), (state s, action a), ...]\n",
    "```\n",
    "\n",
    "In the `dataset.py` file, we provide skeleton code for creating a custom dataset. The provided code shows how to load the file.\n",
    "\n",
    "Your goal is to overwrite the `__getitem__` function in order to return a dictionary of tensors of the correct type.\n",
    "\n",
    "Hint: Look in the `bc.py` file to understand how the dataset is used.\n",
    "\n",
    "Answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**QUESTION 2 points]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER TO INSERT YOUR __getitem__ method here\n",
    "\n",
    "# def __getitem__(self, index):\n",
    "#     item = self.data[index]\n",
    "#     # TODO YOUR CODE HERE\n",
    "#     raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[QUESTION 2 points]** How big is the dataset provided?\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "- **[QUESTION 2 points]** What is the dimensionality of $s$ and what range does each dimension of $s$ span?  I.e., how much of the state space does the expert data cover?\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "- **[QUESTION 2 points]** What are the dimensionalities and ranges of the action $a$ in the dataset (how much of the action space does the expert data cover)?\n",
    "\n",
    "**YOUR ANSWER HERE**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Environment\n",
    "\n",
    "Recall the state and action space of CartPole, from the previous assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[QUESTION 2 points]** Considering the full state and action spaces, do you think the provided expert dataset has good coverage?  Why or why not? How might this impact the performance of our cloned policy?\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Model\n",
    "\n",
    "The file `model.py` provides skeleton code for the model. Your goal is to create the architecture of the network by adding layers that map the input to output.\n",
    "\n",
    "You will need to update the `__init__` method and the `forward` method.\n",
    "\n",
    "The `select_action` method has already been written for you.  This should be used when running the policy in the environment, while the `forward` function should be used at training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**QUESTION 5 points]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER TO INSERT YOUR MyModel class here\n",
    "\n",
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, state_size, action_size):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         # TODO YOUR CODE HERE FOR INITIALIZING THE MODEL\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # TODO YOUR CODE HERE FOR THE FORWARD PASS\n",
    "#         raise NotImplementedError()\n",
    "\n",
    "#     def select_action(self, state):\n",
    "#         self.eval()\n",
    "#         x = self.forward(state)\n",
    "#         self.train()\n",
    "#         return x.max(1)[1].view(1, 1).to(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "- **[QUESTION 2 points]** What is the input of the network?\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "- **[QUESTION 2 points]** What is the output?\n",
    "\n",
    "**YOUR ANSWER HERE**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Training\n",
    "\n",
    "The file `bc.py` is the entry point for training your behavioral cloning model. The skeleton and the main components are already there.\n",
    "\n",
    "The missing parts for you to do are:\n",
    "\n",
    "- Initializing the model\n",
    "- Choosing a loss function\n",
    "- Choosing an optimizer\n",
    "- Playing with hyperparameters to train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**QUESTION 5 points]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER FOR YOUR CODE HER\n",
    "# HOW DID YOU INITIALIZE YOUR MODEL, OPTIMIZER AND LOSS FUNCTIONS? PASTE HERE YOUR FINAL CODE\n",
    "# NOTE: YOU CAN KEEP THE FOLLOWING LINES COMMENTED OUT, AS RUNNING THIS CELL WILL PROBABLY RESULT IN ERRORS\n",
    "\n",
    "# model = None\n",
    "# optimizer = None\n",
    "# loss_function = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run your code by doing:\n",
    "\n",
    "```\n",
    "python3 bc.py\n",
    "```\n",
    "\n",
    "**During all of this assignment, the code in `eval_policy.py` will be your best friend.** At any time, you can test your model by giving as argument the path to the model weights and the environment name using the following command:\n",
    "\n",
    "```\n",
    "python3 eval_policy.py --model-path /path/to/model/weights --env ENV_NAME\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PASTE YOUR TERMINAL OUTPUT HERE\n",
    "# NOTE: TO HAVE LESS LINES PRINTED, YOU CAN SET THE VARIABLE PRINT_INTERVAL TO 5 or 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[QUESTION 2 points]** Did you manage to learn a good policy? How consistent is the reward you are getting?\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Deep Q Learning\n",
    "\n",
    "There are two main issues with the behavior cloning approach.\n",
    "\n",
    "- First, we are not always lucky enough to have access to a dataset of expert demonstrations.\n",
    "- Second, replicating an expert policy suffers from compounding error. The policy $\\pi$ only sees these \"perfect\" examples and has no knowledge on how to recover from states not visited by the expert. For this reason, as soon as it is presented with a state that is off the expert trajectory, it will perform poorly and will continue to deviate from a good trajectory without the possibility of recovering from errors.\n",
    "\n",
    "---\n",
    "The second task consists in solving the environment from scratch, using RL, and most specifically the DQN algorithm, to learn a policy $\\pi$.\n",
    "\n",
    "For this task, familiarize yourself with the file `dqn.py`. We are going to re-use the file `model.py` for the model you created in the previous task.\n",
    "\n",
    "Your task is very similar to the one in the previous assignment, to implement the Q-learning algorithm, but in this version, our Q-function is approximated with a neural network.\n",
    "\n",
    "The algorithm (excerpted from Section 6.5 of [Sutton's book](http://incompleteideas.net/book/RLbook2018.pdf)) is given below:\n",
    "\n",
    "![DQN algorithm](https://i.imgur.com/Mh4Uxta.png)\n",
    "\n",
    "### 2.0 Think about your model...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[QUESTION 2 points]** In DQN, we are using the same model as in task 1 for behavioral cloning. In both tasks the model receives as input the state and in both tasks the model outputs something that has the same dimensionality as the number of actions. These two outputs, though, represent very different things. What is each one representing?\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Update your Q-function\n",
    "\n",
    "Complete the `optimize_model` function. This function receives as input a `state`, an `action`, the `next_state`, the `reward` and `done` representing the tuple $(s_t, a_t, s_{t+1}, r_t, done_t)$. Your task is to update your Q-function as shown in the [Atari DQN paper](https://arxiv.org/abs/1312.5602) environment. For now don't be concerned with the experience replay buffer. We'll get to that later.\n",
    "\n",
    "![Loss function](https://i.imgur.com/tpTsV8m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**QUESTION 8 points]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER TO INSERT YOUR optimize_model function here:\n",
    "\n",
    "# def optimize_model(state, action, next_state, reward, done):\n",
    "#     # TODO given a tuple (s_t, a_t, s_{t+1}, r_t, done_t) update your model weights\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 $\\epsilon$-greedy strategy\n",
    "\n",
    "You will need a strategy to explore your environment. The standard strategy is to use $\\epsilon$-greedy. Implement it in the `choose_action` function template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**QUESTION 5 points]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER TO INSERT YOUR choose_action function here:\n",
    "\n",
    "# def choose_action(state, test_mode=False):\n",
    "#     # TODO implement an epsilon-greedy strategy\n",
    "#     raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train your model\n",
    "\n",
    "Try to train a model in this way.\n",
    "\n",
    "You can run your code by doing:\n",
    "\n",
    "```\n",
    "python3 dqn.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[QUESTION 2 points]** How many episodes does it take to learn (ie. reach a good reward)?\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PASTE YOUR TERMINAL OUTPUT HERE\n",
    "# NOTE: TO HAVE LESS LINES PRINTED, YOU CAN SET THE VARIABLE PRINT_INTERVAL TO 5 or 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Add the Experience Replay Buffer\n",
    "\n",
    "If you read the DQN paper (and as you can see from the algorithm picture above), the authors make use of an experience replay buffer to learn faster. We provide an implementation in the file `replay_buffer.py`. Update the `train_reinforcement_learning` code to push a tuple to the replay buffer and to sample a batch for the `optimize_model` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[QUESTION 5 points]** How does the replay buffer improve performances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PASTE YOUR TERMINAL OUTPUT HERE\n",
    "# NOTE: TO HAVE LESS LINES PRINTED, YOU CAN SET THE VARIABLE PRINT_INTERVAL TO 5 or 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Extra\n",
    "\n",
    "Ideas to experiment with:\n",
    "\n",
    "- Is $\\epsilon$-greedy strategy the best strategy available? Why not trying something different.\n",
    "- Why not make use of the model you have trained in the behavioral cloning part and fine-tune it with RL? How does that affect performance?\n",
    "- You are perhaps bored with `CartPole-v0` by now. Another environment we suggest trying is `LunarLander-v2`. It will be harder to learn but with experimentation, you will find the correct optimizations for success. Piazza is also your friend :)\n",
    "- What about learning from images? This requires more work because you have to extract the image from the environment. However, would it be possible? How much more challenging might you expect the learning to be in this case?\n",
    "- The ReplayBuffer implementation provided is very simple. In class we have briefly mentioned Prioritized Experience Replay; how would the learning process change?\n",
    "- An improvement over DQN is DoubleDQN, which is a very simple addition to the current code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU CAN USE THIS CODEBLOCK AND ADD ANY BLOCK BELOW AS YOU NEED\n",
    "# TO SHOW US THE IDEAS AND EXTRA EXPERIMENTS YOU RUN.\n",
    "# HAVE FUN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
